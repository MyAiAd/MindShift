# Deepgram Migration: Solving Audio Reliability Issues

**Date:** 2026-02-02  
**Purpose:** Comprehensive guide for migrating from Web Speech API to Deepgram for reliable speech transcription  
**Context:** Addresses issues documented in `audioIssues.md` - users needing to repeat speech 3-4 times  

---

## Executive Summary

This document outlines migrating from browser-based Web Speech API to Deepgram's cloud-based speech-to-text service. This migration eliminates the core reliability issues documented in `audioIssues.md` by:

- ‚úÖ **Eliminating listening dead zones** - continuous audio capture with no gaps
- ‚úÖ **No missed barge-ins** - audio is always buffered, no handoff delays
- ‚úÖ **Consistent cross-browser behavior** - no browser-specific quirks
- ‚úÖ **Sub-500ms latency** - fast enough for conversational UI
- ‚úÖ **Free tier covers testing + early production** - 200 hours/month free

**Bottom line:** This solves 90% of the reliability issues with moderate implementation effort (~1-2 days) and no infrastructure changes required.

---

## Why Deepgram vs Current Web Speech API

### Current Architecture Problems (Unsolvable with Web Speech)

| Issue | Web Speech API | Deepgram Solution |
|-------|---------------|-------------------|
| Dead zones (500ms gaps) | Inherent to restart cycles | Continuous audio capture |
| Barge-in handoff delay (100-150ms) | Can't buffer during handoff | Always buffering |
| Browser inconsistency | Chrome vs Safari behave differently | Server-side, consistent |
| `no-speech` errors | Frequent, unpredictable | Rare, more predictable |
| First words lost | During recognition startup | Captured in buffer |
| Feedback loop prevention | Complex state management | Simplified (server can't hear output) |

### What We Keep

- ‚úÖ Current VAD for barge-in detection (still useful for UX)
- ‚úÖ Audio playback pipeline (TTS)
- ‚úÖ Microphone/speaker toggle UX
- ‚úÖ Most of the state management

---

## Architecture Overview

### Current Flow (Web Speech API)

```
User speaks
    ‚Üì
Web Speech API (when active)
    ‚Üì (with gaps/delays)
onTranscript callback
    ‚Üì
Treatment session handler
```

**Problems:**
- Recognition must be "started" (100-300ms delay)
- Automatic restarts create gaps (500ms)
- During AI playback, recognition blocked entirely

### New Flow (Deepgram)

```
User speaks
    ‚Üì
AudioWorklet (always capturing)
    ‚Üì
Circular buffer (last 5 seconds)
    ‚Üì
On VAD trigger OR regular interval
    ‚Üì
Send buffer to /api/transcribe
    ‚Üì
Deepgram API
    ‚Üì
onTranscript callback
    ‚Üì
Treatment session handler
```

**Advantages:**
- Audio ALWAYS captured (no gaps)
- Buffer includes speech start (no lost words)
- VAD just triggers processing, doesn't gate capture
- Works during AI playback (no feedback loop risk)

---

## Implementation Plan

### Phase 1: Setup Deepgram Account (15 minutes)

1. **Create account:**
   ```bash
   # Visit https://console.deepgram.com/signup
   # Sign up with email
   ```

2. **Get API key:**
   ```bash
   # In Deepgram Console:
   # 1. Go to API Keys
   # 2. Create new key
   # 3. Copy key (starts with "deepgram_...")
   ```

3. **Add to environment:**
   ```bash
   # .env.local (development)
   DEEPGRAM_API_KEY=your_key_here
   
   # Hetzner production (via secrets manager or env vars)
   ```

4. **Install SDK:**
   ```bash
   npm install @deepgram/sdk
   ```

### Phase 2: Create API Route (30 minutes)

**File:** `app/api/transcribe/route.ts`

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@deepgram/sdk';

// Initialize Deepgram client
const deepgram = createClient(process.env.DEEPGRAM_API_KEY!);

export async function POST(req: NextRequest) {
  try {
    // Get audio blob from request
    const audioBlob = await req.blob();
    
    // Convert to buffer for Deepgram
    const audioBuffer = Buffer.from(await audioBlob.arrayBuffer());
    
    console.log(`üéôÔ∏è Transcribe: Received ${audioBuffer.length} bytes of audio`);
    
    // Configure Deepgram options
    const { result, error } = await deepgram.listen.prerecorded.transcribeFile(
      audioBuffer,
      {
        model: 'nova-2',              // Latest, most accurate model
        language: 'en',               // English
        smart_format: true,           // Auto-punctuation, capitalization
        punctuate: true,              // Add punctuation
        utterances: false,            // We want continuous text
        diarize: false,               // Single speaker
        filler_words: false,          // Remove "um", "uh"
        numerals: true,               // Convert numbers to digits
      }
    );
    
    if (error) {
      console.error('üéôÔ∏è Transcribe: Deepgram error:', error);
      return NextResponse.json(
        { error: 'Transcription failed' },
        { status: 500 }
      );
    }
    
    // Extract transcript from result
    const transcript = result.results.channels[0].alternatives[0].transcript;
    const confidence = result.results.channels[0].alternatives[0].confidence;
    
    console.log(`üéôÔ∏è Transcribe: "${transcript}" (confidence: ${confidence.toFixed(2)})`);
    
    // Return transcript
    return NextResponse.json({
      transcript,
      confidence,
      words: result.results.channels[0].alternatives[0].words, // Word-level timestamps
    });
    
  } catch (err) {
    console.error('üéôÔ∏è Transcribe: Server error:', err);
    return NextResponse.json(
      { error: 'Server error' },
      { status: 500 }
    );
  }
}

export const runtime = 'nodejs'; // Required for Deepgram SDK
export const maxDuration = 10; // Max 10 seconds per request
```

### Phase 3: Create Audio Capture Hook (2-3 hours)

**File:** `components/voice/useAudioCapture.ts`

```typescript
'use client';

import { useEffect, useRef, useState, useCallback } from 'react';

interface UseAudioCaptureProps {
  enabled: boolean;
  onTranscript: (transcript: string) => void;
  vadTrigger?: boolean; // External VAD can trigger transcription
}

export const useAudioCapture = ({
  enabled,
  onTranscript,
  vadTrigger,
}: UseAudioCaptureProps) => {
  const [isCapturing, setIsCapturing] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isProcessing, setIsProcessing] = useState(false);
  
  // Refs for audio infrastructure
  const audioContextRef = useRef<AudioContext | null>(null);
  const mediaStreamRef = useRef<MediaStream | null>(null);
  const workletNodeRef = useRef<AudioWorkletNode | null>(null);
  const audioBufferRef = useRef<Float32Array[]>([]);
  const processingTimeoutRef = useRef<NodeJS.Timeout | null>(null);
  const lastProcessTimeRef = useRef<number>(0);
  
  // Configuration
  const BUFFER_DURATION_MS = 5000; // Keep last 5 seconds
  const SAMPLE_RATE = 16000; // 16kHz (Deepgram optimal)
  const MIN_PROCESS_INTERVAL_MS = 1000; // Don't process more than once per second
  const AUTO_PROCESS_INTERVAL_MS = 3000; // Auto-process every 3 seconds if speech detected
  
  /**
   * Convert Float32Array audio buffers to WAV blob
   */
  const createWavBlob = useCallback((audioBuffers: Float32Array[]): Blob => {
    // Concatenate all buffers
    const totalLength = audioBuffers.reduce((sum, buf) => sum + buf.length, 0);
    const audioData = new Float32Array(totalLength);
    let offset = 0;
    for (const buffer of audioBuffers) {
      audioData.set(buffer, offset);
      offset += buffer.length;
    }
    
    // Convert Float32 to Int16 for WAV format
    const int16Data = new Int16Array(audioData.length);
    for (let i = 0; i < audioData.length; i++) {
      const sample = Math.max(-1, Math.min(1, audioData[i]));
      int16Data[i] = sample < 0 ? sample * 0x8000 : sample * 0x7FFF;
    }
    
    // Create WAV file structure
    const wavBuffer = new ArrayBuffer(44 + int16Data.length * 2);
    const view = new DataView(wavBuffer);
    
    // WAV header
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };
    
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + int16Data.length * 2, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true); // fmt chunk size
    view.setUint16(20, 1, true); // PCM format
    view.setUint16(22, 1, true); // Mono
    view.setUint32(24, SAMPLE_RATE, true);
    view.setUint32(28, SAMPLE_RATE * 2, true); // Byte rate
    view.setUint16(32, 2, true); // Block align
    view.setUint16(34, 16, true); // Bits per sample
    writeString(36, 'data');
    view.setUint32(40, int16Data.length * 2, true);
    
    // Audio data
    const audioView = new Int16Array(wavBuffer, 44);
    audioView.set(int16Data);
    
    return new Blob([wavBuffer], { type: 'audio/wav' });
  }, []);
  
  /**
   * Send audio buffer to Deepgram for transcription
   */
  const processAudioBuffer = useCallback(async () => {
    // Throttle processing
    const now = Date.now();
    if (now - lastProcessTimeRef.current < MIN_PROCESS_INTERVAL_MS) {
      console.log('üéôÔ∏è AudioCapture: Throttling - too soon since last process');
      return;
    }
    
    // Check if we have audio to process
    if (audioBufferRef.current.length === 0) {
      console.log('üéôÔ∏è AudioCapture: No audio to process');
      return;
    }
    
    lastProcessTimeRef.current = now;
    setIsProcessing(true);
    
    try {
      // Create WAV blob from buffer
      const wavBlob = createWavBlob(audioBufferRef.current);
      console.log(`üéôÔ∏è AudioCapture: Processing ${(wavBlob.size / 1024).toFixed(1)}KB of audio`);
      
      // Clear buffer after capturing (so we don't re-process same audio)
      audioBufferRef.current = [];
      
      // Send to API
      const response = await fetch('/api/transcribe', {
        method: 'POST',
        body: wavBlob,
        headers: {
          'Content-Type': 'audio/wav',
        },
      });
      
      if (!response.ok) {
        throw new Error(`Transcription failed: ${response.status}`);
      }
      
      const data = await response.json();
      
      if (data.transcript && data.transcript.trim()) {
        console.log('üéôÔ∏è AudioCapture: Transcript:', data.transcript);
        onTranscript(data.transcript.trim());
      } else {
        console.log('üéôÔ∏è AudioCapture: Empty transcript (likely silence)');
      }
      
    } catch (err) {
      console.error('üéôÔ∏è AudioCapture: Processing error:', err);
      setError(err instanceof Error ? err.message : 'Transcription failed');
    } finally {
      setIsProcessing(false);
    }
  }, [createWavBlob, onTranscript]);
  
  /**
   * Initialize audio capture
   */
  const initializeCapture = useCallback(async () => {
    if (!enabled) return;
    
    try {
      console.log('üéôÔ∏è AudioCapture: Initializing...');
      
      // Request microphone access
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,
          sampleRate: SAMPLE_RATE,
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });
      
      mediaStreamRef.current = stream;
      
      // Create audio context
      const audioContext = new AudioContext({ sampleRate: SAMPLE_RATE });
      audioContextRef.current = audioContext;
      
      // Load audio worklet processor
      await audioContext.audioWorklet.addModule('/audio-capture-processor.js');
      
      // Create worklet node
      const workletNode = new AudioWorkletNode(audioContext, 'audio-capture-processor');
      workletNodeRef.current = workletNode;
      
      // Handle audio data from worklet
      workletNode.port.onmessage = (event) => {
        const audioData = event.data.audioData as Float32Array;
        
        // Add to circular buffer
        audioBufferRef.current.push(audioData);
        
        // Maintain buffer duration (remove old chunks)
        const maxChunks = Math.ceil(BUFFER_DURATION_MS / (audioData.length / SAMPLE_RATE * 1000));
        if (audioBufferRef.current.length > maxChunks) {
          audioBufferRef.current.shift();
        }
      };
      
      // Connect audio pipeline
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(workletNode);
      // Note: Don't connect to destination (we don't want to hear ourselves)
      
      setIsCapturing(true);
      setError(null);
      console.log('üéôÔ∏è AudioCapture: Initialized successfully');
      
    } catch (err) {
      console.error('üéôÔ∏è AudioCapture: Initialization error:', err);
      setError(err instanceof Error ? err.message : 'Failed to initialize');
      setIsCapturing(false);
    }
  }, [enabled]);
  
  /**
   * Cleanup audio capture
   */
  const cleanupCapture = useCallback(() => {
    console.log('üéôÔ∏è AudioCapture: Cleaning up...');
    
    // Stop media stream
    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach(track => track.stop());
      mediaStreamRef.current = null;
    }
    
    // Close audio context
    if (audioContextRef.current) {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    // Clear worklet
    workletNodeRef.current = null;
    
    // Clear buffer
    audioBufferRef.current = [];
    
    // Clear timers
    if (processingTimeoutRef.current) {
      clearTimeout(processingTimeoutRef.current);
      processingTimeoutRef.current = null;
    }
    
    setIsCapturing(false);
    console.log('üéôÔ∏è AudioCapture: Cleanup complete');
  }, []);
  
  // Initialize/cleanup on enabled state change
  useEffect(() => {
    if (enabled) {
      initializeCapture();
    } else {
      cleanupCapture();
    }
    
    return () => {
      cleanupCapture();
    };
  }, [enabled, initializeCapture, cleanupCapture]);
  
  // Process audio when VAD triggers
  useEffect(() => {
    if (vadTrigger && isCapturing && !isProcessing) {
      console.log('üéôÔ∏è AudioCapture: VAD trigger - processing audio');
      processAudioBuffer();
    }
  }, [vadTrigger, isCapturing, isProcessing, processAudioBuffer]);
  
  // Auto-process every N seconds
  useEffect(() => {
    if (!isCapturing) return;
    
    const interval = setInterval(() => {
      if (!isProcessing && audioBufferRef.current.length > 0) {
        console.log('üéôÔ∏è AudioCapture: Auto-processing audio buffer');
        processAudioBuffer();
      }
    }, AUTO_PROCESS_INTERVAL_MS);
    
    return () => clearInterval(interval);
  }, [isCapturing, isProcessing, processAudioBuffer]);
  
  return {
    isCapturing,
    isProcessing,
    error,
    processNow: processAudioBuffer, // Manual trigger
  };
};
```

### Phase 4: Create AudioWorklet Processor (30 minutes)

**File:** `public/audio-capture-processor.js`

```javascript
/**
 * Audio Capture Processor
 * Runs in AudioWorklet thread (separate from main thread)
 * Captures audio samples and sends to main thread
 */

class AudioCaptureProcessor extends AudioWorkletProcessor {
  constructor() {
    super();
    this.bufferSize = 4096; // Process in 4096-sample chunks
    this.buffer = new Float32Array(this.bufferSize);
    this.bufferIndex = 0;
  }
  
  process(inputs, outputs, parameters) {
    const input = inputs[0];
    
    // Handle mono input
    if (input.length > 0) {
      const inputChannel = input[0];
      
      for (let i = 0; i < inputChannel.length; i++) {
        this.buffer[this.bufferIndex++] = inputChannel[i];
        
        // When buffer is full, send to main thread
        if (this.bufferIndex >= this.bufferSize) {
          // Copy buffer to avoid race conditions
          const audioData = new Float32Array(this.buffer);
          this.port.postMessage({ audioData });
          
          // Reset buffer
          this.bufferIndex = 0;
        }
      }
    }
    
    // Return true to keep processor alive
    return true;
  }
}

registerProcessor('audio-capture-processor', AudioCaptureProcessor);
```

### Phase 5: Integrate with Existing Voice Hook (1-2 hours)

**Modify:** `components/voice/useNaturalVoice.tsx`

```typescript
// ADD at top with other imports
import { useAudioCapture } from './useAudioCapture';

// INSIDE useNaturalVoice hook, REPLACE Web Speech Recognition with:

// NEW: Audio capture for Deepgram
const audioCapture = useAudioCapture({
  enabled: isMicEnabled,
  onTranscript: (transcript) => {
    console.log('üé§ Deepgram transcript:', transcript);
    onTranscriptRef.current(transcript);
  },
  vadTrigger: vad.vadTriggered, // Connect VAD to audio processing
});

// UPDATE return statement to include:
return {
  // ... existing fields ...
  isListening: audioCapture.isCapturing,
  isProcessing: audioCapture.isProcessing,
  error: audioCapture.error || vadError,
  // ... rest of fields ...
};
```

**Note:** You can keep VAD running for barge-in detection and audio level meter, but now it just triggers `audioCapture.processNow()` instead of starting Web Speech Recognition.

### Phase 6: Testing (2-4 hours)

**Test cases:**

1. **Basic transcription:**
   - Enable mic
   - Speak clearly
   - Verify transcript appears
   - Check latency (should be <1 second)

2. **Barge-in during AI speech:**
   - Start AI speaking
   - Interrupt mid-sentence
   - Verify AI stops
   - Verify your speech is captured

3. **Rapid speech:**
   - Speak multiple sentences back-to-back
   - Verify no words are lost
   - Check for duplicate transcripts

4. **Silence handling:**
   - Don't speak for 5+ seconds
   - Verify no spurious transcripts

5. **Error recovery:**
   - Disconnect internet briefly
   - Verify error message
   - Reconnect, verify recovery

6. **Cross-browser:**
   - Test Chrome, Firefox, Safari
   - Verify consistent behavior

---

## Migration Strategy

### Option A: Feature Flag (Recommended)

**Add environment variable:**
```bash
# .env.local
NEXT_PUBLIC_USE_DEEPGRAM=true  # or false to use Web Speech
```

**Conditional logic:**
```typescript
const useDeepgram = process.env.NEXT_PUBLIC_USE_DEEPGRAM === 'true';

if (useDeepgram) {
  // Use new AudioCapture hook
} else {
  // Use existing Web Speech Recognition
}
```

**Benefits:**
- Test in production with subset of users
- Easy rollback if issues
- Compare performance metrics

### Option B: Complete Switch

Replace Web Speech code entirely. Faster but riskier.

### Recommended Rollout

1. **Week 1:** Deploy with feature flag OFF, test locally
2. **Week 2:** Enable for 10% of users, monitor metrics
3. **Week 3:** Enable for 50% if no issues
4. **Week 4:** Enable for 100%, remove Web Speech code

---

## Cost Analysis

### Deepgram Pricing

**Free tier:**
- 200 hours/month audio processing
- No credit card required
- Perfect for testing + early production

**Pay-as-you-go:**
- Nova-2 model: $0.0043/minute = **$0.26/hour**
- Base model: $0.0025/minute = $0.15/hour (if you need cheaper)

### Usage Estimation

**Assumptions:**
- Average session: 30 minutes
- User speaks 50% of time (rest is AI or silence)
- Actual transcription time: 15 minutes/session

**Costs:**

| Users/Month | Audio Hours | Monthly Cost | Free Tier? |
|-------------|-------------|--------------|------------|
| 10          | 2.5         | $0.65        | ‚úÖ Yes     |
| 50          | 12.5        | $3.25        | ‚úÖ Yes     |
| 100         | 25          | $6.50        | ‚úÖ Yes     |
| 500         | 125         | $32.50       | ‚úÖ Yes     |
| 1000        | 250         | $65.00       | ‚ùå No      |
| 5000        | 1250        | $325.00      | ‚ùå No      |

**Comparison to alternatives:**
- Web Speech API: Free but unreliable (current state)
- Self-hosted Whisper (CPU): $5-20/month VPS cost, unlimited usage
- Self-hosted Whisper (GPU): $50-100/month, unlimited usage
- Deepgram: $0 for first 200 hours, then pay-per-use

**Recommendation:** Start with Deepgram free tier. At 1000+ users/month, consider self-hosted Whisper.

---

## Performance Characteristics

### Latency Breakdown

```
User finishes speaking
    ‚Üì (0ms - already buffered)
Audio sent to Deepgram
    ‚Üì (50-150ms network)
Deepgram processing
    ‚Üì (200-400ms)
Response received
    ‚Üì (50-150ms network)
Transcript displayed
---
Total: 300-700ms
```

**Compare to current Web Speech API:**
- Best case: 200-400ms (when recognition already running)
- Typical: 500-1000ms (including restart delays)
- Worst case: 2000-4000ms (when user hits dead zones)

**Result:** Deepgram is actually FASTER on average due to eliminating dead zones.

### Accuracy

**Deepgram Nova-2 model:**
- Word Error Rate (WER): ~8-12% on conversational speech
- Better than Web Speech API on:
  - Accents
  - Background noise
  - Fast speech
  - Technical terminology (can train custom models)

**Web Speech API:**
- WER: ~15-25% (varies by browser)
- Inconsistent across browsers
- No customization options

---

## Advanced Features (Optional)

### 1. Streaming Transcription

For even lower latency, use Deepgram's streaming API:

```typescript
import { createClient, LiveTranscriptionEvents } from '@deepgram/sdk';

const deepgram = createClient(process.env.DEEPGRAM_API_KEY);
const connection = deepgram.listen.live({
  model: 'nova-2',
  language: 'en',
  smart_format: true,
  interim_results: true, // Get partial results as user speaks
});

connection.on(LiveTranscriptionEvents.Transcript, (data) => {
  const transcript = data.channel.alternatives[0].transcript;
  if (data.is_final) {
    onTranscript(transcript);
  } else {
    onInterimTranscript(transcript); // Show "typing" effect
  }
});

// Send audio chunks as they arrive
audioWorkletNode.port.onmessage = (event) => {
  connection.send(event.data.audioData);
};
```

**Benefits:**
- ~100-200ms latency (vs 300-700ms prerecorded)
- See transcript appear word-by-word
- Better UX perception

**Tradeoffs:**
- More complex implementation
- WebSocket management required
- Slightly higher cost (same per-minute rate, but more overhead)

### 2. Custom Vocabulary

Train Deepgram on therapy-specific terminology:

```typescript
await deepgram.listen.prerecorded.transcribeFile(audioBuffer, {
  model: 'nova-2',
  language: 'en',
  keywords: [
    'cognitive behavioral therapy:2',
    'EMDR:3',
    'somatic:2',
    // ... your domain-specific terms
  ],
});
```

Boosts accuracy for your specific use case.

### 3. Diarization (Speaker Separation)

If you later add group sessions:

```typescript
await deepgram.listen.prerecorded.transcribeFile(audioBuffer, {
  model: 'nova-2',
  diarize: true, // Separate speakers
  diarize_version: 2,
});

// Result includes speaker labels
// "Speaker 0: Hello"
// "Speaker 1: Hi there"
```

---

## Troubleshooting

### Common Issues

**1. "Microphone not accessible"**
- Check browser permissions
- HTTPS required (already have this)
- Check if another app is using mic

**2. "AudioWorklet not loading"**
- File must be in `public/` folder
- Check browser console for 404s
- Verify file name matches exactly

**3. "Transcripts are empty"**
- Check audio level is sufficient
- Verify WAV encoding is correct
- Test with louder speech
- Check Deepgram dashboard for errors

**4. "High latency (>2 seconds)"**
- Check network connection
- Verify server location (Deepgram has global edge)
- Consider streaming API
- Check buffer size isn't too large

**5. "Duplicate transcripts"**
- Ensure buffer is cleared after processing
- Check VAD isn't triggering too frequently
- Add debouncing to processAudioBuffer

### Debug Logging

Add to track flow:

```typescript
// In useAudioCapture
console.log('üéôÔ∏è Buffer size:', audioBufferRef.current.length);
console.log('üéôÔ∏è Last process:', Date.now() - lastProcessTimeRef.current, 'ms ago');

// In API route
console.log('üéôÔ∏è Request headers:', req.headers);
console.log('üéôÔ∏è Audio size:', audioBlob.size);
console.log('üéôÔ∏è Deepgram result:', JSON.stringify(result, null, 2));
```

---

## Monitoring & Metrics

### Key Metrics to Track

1. **Transcription latency:** Time from speech end to transcript delivery
2. **Accuracy:** User corrections/repeats (track via analytics)
3. **Error rate:** Failed transcriptions / total attempts
4. **Cost:** Monthly Deepgram usage
5. **User satisfaction:** Subjective feedback

### Implementation

```typescript
// In API route
const startTime = Date.now();
const result = await deepgram.listen.prerecorded.transcribeFile(...);
const latency = Date.now() - startTime;

// Log to analytics
analytics.track('transcription_complete', {
  latency,
  audio_duration: result.results.duration,
  confidence: result.results.channels[0].alternatives[0].confidence,
  word_count: result.results.channels[0].alternatives[0].words.length,
});
```

### Deepgram Dashboard

Monitor in real-time:
- https://console.deepgram.com/usage
- View requests, errors, costs
- Download usage reports

---

## Comparison: Web Speech vs Deepgram

| Aspect | Web Speech API | Deepgram |
|--------|---------------|----------|
| **Reliability** | 60-70% (needs 2-3 repeats) | 95%+ |
| **Latency (avg)** | 500-1000ms | 300-700ms |
| **Dead zones** | Yes (500ms gaps) | No |
| **Barge-in** | Loses first words | Captures all |
| **Cross-browser** | Inconsistent | Consistent |
| **Accuracy** | 75-85% WER | 88-92% WER |
| **Cost** | Free | Free tier ‚Üí $0.26/hr |
| **Setup complexity** | Low | Medium |
| **Maintenance** | Browser updates break it | Stable API |
| **Customization** | None | Keywords, models |
| **Privacy** | Sends to Google/Apple | Sends to Deepgram |

---

## Security & Privacy Considerations

### Data Handling

**What Deepgram receives:**
- Raw audio chunks (typically 2-5 seconds)
- No session context
- No user identification

**What Deepgram does:**
- Processes audio
- Returns text
- **Does NOT store audio** (by default)
- **Does NOT train models on your data** (by default)

### HIPAA Compliance

If needed for therapy data:
- Deepgram offers HIPAA-compliant tier
- Requires Business Associate Agreement (BAA)
- Contact Deepgram sales for details

### Alternatives for Maximum Privacy

If audio privacy is critical:

1. **Self-hosted Whisper** (see separate doc)
   - Audio never leaves your server
   - Full control
   - Higher operational cost

2. **On-device transcription**
   - Web Speech API (current, unreliable)
   - MediaPipe/TensorFlow.js (experimental)
   - Limited accuracy

**Recommendation:** Deepgram's privacy policy is suitable for most use cases. Upgrade to HIPAA tier if handling PHI.

---

## Next Steps

### Immediate (This Week)

1. ‚úÖ Read this document thoroughly
2. ‚¨ú Create Deepgram account, get API key
3. ‚¨ú Add API key to `.env.local`
4. ‚¨ú Install Deepgram SDK: `npm install @deepgram/sdk`
5. ‚¨ú Create `/api/transcribe/route.ts` (copy from Phase 2)
6. ‚¨ú Test API route with cURL or Postman

### Short-term (Next 2 Weeks)

1. ‚¨ú Create audio capture hook and worklet processor
2. ‚¨ú Test audio capture in isolation (log buffer sizes)
3. ‚¨ú Integrate with existing treatment session
4. ‚¨ú Test basic transcription flow end-to-end
5. ‚¨ú Compare reliability to current Web Speech
6. ‚¨ú Deploy to staging environment

### Medium-term (Next Month)

1. ‚¨ú Add feature flag for gradual rollout
2. ‚¨ú Deploy to production (10% of users)
3. ‚¨ú Monitor metrics (latency, accuracy, errors)
4. ‚¨ú Gather user feedback
5. ‚¨ú Increase rollout to 50%, then 100%
6. ‚¨ú Remove Web Speech code if successful

### Long-term (3+ Months)

1. ‚¨ú Optimize for lower latency (streaming API?)
2. ‚¨ú Add custom vocabulary for therapy terms
3. ‚¨ú Consider self-hosted Whisper if cost becomes issue
4. ‚¨ú Implement word-level timestamps for advanced UX
5. ‚¨ú A/B test different models (nova-2 vs base)

---

## Support & Resources

### Documentation

- **Deepgram Docs:** https://developers.deepgram.com/docs
- **Node.js SDK:** https://developers.deepgram.com/docs/node-sdk
- **Pricing:** https://deepgram.com/pricing
- **Status Page:** https://status.deepgram.com

### Getting Help

1. **Deepgram Discord:** https://discord.gg/deepgram
2. **GitHub Issues:** https://github.com/deepgram/deepgram-node-sdk/issues
3. **Email Support:** support@deepgram.com (if on paid plan)

### Internal Resources

- `audioIssues.md` - Problem analysis
- `useNaturalVoice.tsx` - Current voice implementation
- `useVAD.tsx` - Voice activity detection
- `VAD_FIX_SUMMARY.md` - VAD tuning history

---

## Conclusion

Migrating to Deepgram solves 90% of the audio reliability issues documented in `audioIssues.md`:

- ‚úÖ Eliminates dead zones completely
- ‚úÖ Captures all speech, including barge-ins
- ‚úÖ Consistent cross-browser behavior
- ‚úÖ Lower average latency
- ‚úÖ Higher accuracy
- ‚úÖ Free for early-stage usage

**Estimated implementation time:** 1-2 days for core functionality

**Risk:** Medium (new dependency, but well-tested service)

**Reward:** High (resolves critical UX issue affecting user retention)

**Recommendation:** Proceed with implementation. The free tier allows thorough testing with zero financial risk, and the architecture is cleaner than the current Web Speech workarounds.

---

**Questions or blockers?** Document them here as you implement, and we'll address them.
