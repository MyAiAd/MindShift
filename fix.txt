# Voice Treatment Demo Script Adherence Fix

## Executive Summary

The Voice Treatment Demo is not following the scripted responses from the treatment state machine because of fundamental misuse of the OpenAI Realtime API. The current approach attempts to control responses using `response.create` with custom instructions, which is not how the API is designed to work.

**Root Cause:** The OpenAI Realtime API doesn't support overriding system behavior with response-level instructions. The AI will still generate its own responses based on conversation context, ignoring the instructions provided in `response.create`.

**Solution:** Use `conversation.item.create` to manually add assistant messages with exact script content to the conversation, then use `response.create` to trigger audio generation of those messages.

## Root Cause Analysis

### Current Broken Approach
```javascript
// ❌ THIS DOESN'T WORK
const responseMessage = {
  type: 'response.create',
  response: {
    modalities: ['audio', 'text'],
    instructions: `Speak exactly and only this text: "${scriptedResponse}". Do not add any other words before or after.`
  }
};
```

**Why it fails:**
- The `instructions` field in `response.create` is not meant to override system instructions
- The AI still generates responses based on conversation context, not response-level instructions
- Cancellation logic conflicts with manual response creation

### Correct Approach
```javascript
// ✅ THIS WORKS
// Step 1: Add assistant message to conversation
const assistantMessage = {
  type: 'conversation.item.create',
  item: {
    type: 'message',
    role: 'assistant',
    status: 'completed',
    content: [{
      type: 'text',
      text: scriptedResponse
    }]
  }
};

// Step 2: Trigger audio generation of that message
const responseCreate = {
  type: 'response.create',
  response: {
    modalities: ['audio']
  }
};
```

## Complete Implementation Fix

### 1. Replace the `createManualResponse` Function

```javascript
// NEW: Correct script-adherent response creation
const createScriptedVoiceResponse = async (scriptedResponse, userTranscript) => {
  if (sessionRef.current.dataChannel?.readyState !== 'open') {
    console.error('🔍 VOICE_DEBUG: DataChannel not ready');
    return;
  }
  
  try {
    console.log(`🔍 VOICE_DEBUG: Creating scripted response: "${scriptedResponse}"`);
    
    // Step 1: Create assistant message with exact script
    const assistantMessageEvent = {
      type: 'conversation.item.create',
      item: {
        type: 'message',
        role: 'assistant',
        status: 'completed',
        content: [{
          type: 'text',
          text: scriptedResponse
        }]
      }
    };
    
    sessionRef.current.dataChannel.send(JSON.stringify(assistantMessageEvent));
    console.log(`🔍 VOICE_DEBUG: Assistant message added to conversation`);
    
    // Step 2: Wait briefly then trigger audio response
    setTimeout(() => {
      const responseEvent = {
        type: 'response.create',
        response: {
          modalities: ['audio']
          // No instructions - uses the assistant message we just added
        }
      };
      
      sessionRef.current.dataChannel?.send(JSON.stringify(responseEvent));
      console.log(`🔍 VOICE_DEBUG: Audio response triggered`);
    }, 150);
    
    // Update UI
    addMessage(scriptedResponse, false, true);
    
  } catch (error) {
    console.error(`🔍 VOICE_DEBUG: Failed to create scripted response:`, error);
  }
};
```

### 2. Fix Session Configuration

```javascript
// CRITICAL: Correct session configuration for script adherence
const sessionConfig = {
  type: 'session.update',
  session: {
    instructions: `You are conducting a Mind Shifting treatment session. You must speak ONLY the exact text provided in assistant messages. Never generate original content. Never add words. Speak exactly what is provided, nothing more.`,
    voice: 'verse',
    input_audio_transcription: {
      model: 'whisper-1'
    },
    // CRITICAL: Disable automatic turn detection completely
    turn_detection: null, // This prevents automatic response generation
    modalities: ['text', 'audio'],
    // CRITICAL: Minimum temperature for maximum consistency
    temperature: 0.0,
    max_response_output_tokens: 150,
    tools: [] // No tools to prevent unexpected function calling
  }
};
```

### 3. Update the OpenAI Session Creation API Call

```javascript
// In /api/labs/openai-session route
const sessionResponse = await fetch('/api/labs/openai-session', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'gpt-4o-realtime-preview-2024-12-17',
    voice: 'verse',
    // CRITICAL: System instructions for script adherence
    instructions: `You are a Mind Shifting treatment assistant. You must only speak the exact text content from assistant messages in the conversation. Never generate original responses. Never add extra words, introductions, or explanations. Speak only what is explicitly provided in assistant message content.`,
    
    input_audio_transcription: {
      model: 'whisper-1'
    },
    
    // CRITICAL: Disable automatic turn detection
    turn_detection: null,
    
    modalities: ['text', 'audio'],
    temperature: 0.0,
    max_response_output_tokens: 150
  })
});
```

### 4. Simplify the Message Handler

```javascript
// UPDATED: Simplified message handler focused on script adherence
const handleRealtimeMessage = (event) => {
  try {
    const message = JSON.parse(event.data);
    const messageType = message.type;
    
    console.log(`🔍 VOICE_DEBUG: Received: ${messageType}`);
    
    switch (messageType) {
      case 'input_audio_buffer.speech_started':
        console.log(`🔍 VOICE_DEBUG: User started speaking`);
        break;
        
      case 'input_audio_buffer.speech_stopped':
        console.log(`🔍 VOICE_DEBUG: User stopped speaking`);
        break;
        
      case 'conversation.item.input_audio_transcription.completed':
        const transcript = message.transcript?.trim();
        if (transcript) {
          console.log(`🔍 VOICE_DEBUG: Transcription: "${transcript}"`);
          addMessage(transcript, true, true);
          updateContextFromTranscript(transcript);
          
          // Process with state machine and create scripted response
          processTranscriptWithStateMachine(transcript);
        }
        break;
        
      case 'response.created':
        console.log(`🔍 VOICE_DEBUG: Response created - should be scripted`);
        break;
        
      case 'response.done':
        console.log(`🔍 VOICE_DEBUG: Response completed`);
        break;
        
      case 'conversation.item.created':
        const role = message.item?.role;
        const content = message.item?.content?.[0]?.text;
        console.log(`🔍 VOICE_DEBUG: Item created - ${role}: "${content}"`);
        break;
        
      case 'error':
        console.error(`🔍 VOICE_DEBUG: API Error:`, message);
        setError(`API Error: ${message.error?.message || 'Unknown error'}`);
        break;
        
      default:
        console.log(`🔍 VOICE_DEBUG: Other message: ${messageType}`);
    }
    
  } catch (err) {
    console.log(`🔍 VOICE_DEBUG: Non-JSON message received`);
  }
};
```

### 5. Update State Machine Integration

```javascript
// UPDATED: State machine processing with correct response creation
const processTranscriptWithStateMachine = async (transcript) => {
  if (!stateMachineDemo) {
    console.log(`🔍 VOICE_DEBUG: State machine not initialized`);
    return null;
  }
  
  console.log(`🔍 VOICE_DEBUG: Processing: "${transcript}"`);
  
  try {
    const result = await stateMachineDemo.processUserInput(transcript, undefined, true);
    console.log(`🔍 VOICE_DEBUG: State machine result:`, result);
    
    if (result.scriptedResponse) {
      console.log(`🔍 VOICE_DEBUG: Script: "${result.scriptedResponse}"`);
      
      // Use the correct script creation method
      await createScriptedVoiceResponse(result.scriptedResponse, transcript);
      
      return result.scriptedResponse;
    }
  } catch (error) {
    console.error(`🔍 VOICE_DEBUG: Processing error:`, error);
  }
  
  return null;
};
```

### 6. Remove All Response Cancellation Logic

```javascript
// ❌ REMOVE ALL OF THIS CODE - it conflicts with manual control
/*
// DELETE THESE SECTIONS:
- All pendingManualResponses logic
- All response.cancel messages  
- All response.stop messages
- All automatic response cancellation in message handlers
*/

// ✅ KEEP ONLY: conversation.item.create → response.create pattern
```

## Step-by-Step Implementation Guide

### Phase 1: Update Session Configuration
1. Modify `/api/labs/openai-session` to use the corrected session parameters
2. Set `turn_detection: null` to disable automatic responses
3. Set `temperature: 0.0` for consistency
4. Update system instructions to emphasize exact adherence

### Phase 2: Replace Response Creation Logic
1. Replace the `createManualResponse` function with `createScriptedVoiceResponse`
2. Remove all response cancellation logic
3. Update the message handler to use the simplified version
4. Ensure `conversation.item.create` is sent before `response.create`

### Phase 3: Update State Machine Integration
1. Modify `processTranscriptWithStateMachine` to use the new response creation
2. Ensure proper error handling and logging
3. Test with simple scripted responses first

### Phase 4: Testing and Validation
1. Use the debugging functions to verify script adherence
2. Test with unique phrases to confirm exact matching
3. Monitor conversation items to ensure proper message creation

## Testing and Debugging

### Test Script Adherence
```javascript
const testScriptAdherence = () => {
  console.log('🔍 DEBUG: Testing script adherence...');
  
  // Use a very specific, unique phrase for testing
  const testScript = "Purple elephant dancing on Tuesday at 3:47 PM exactly.";
  
  if (sessionRef.current.dataChannel?.readyState === 'open') {
    createScriptedVoiceResponse(testScript, "test");
  }
  
  // Listen for this exact phrase in the audio output
  // If AI says anything different, script adherence is broken
};
```

### Monitor Conversation State
```javascript
// Add to message handler for debugging
if (message.type === 'conversation.item.created') {
  console.log('🔍 CONVERSATION_ITEM:', {
    id: message.item?.id,
    role: message.item?.role,
    content: message.item?.content?.[0]?.text,
    status: message.item?.status
  });
}

if (message.type === 'response.audio_transcript.delta') {
  console.log('🔍 AI_SPEAKING:', message.delta);
  // Compare this to your expected script
}
```

## Critical Configuration Checklist

### ✅ Required Settings
- [ ] `turn_detection: null` in session configuration
- [ ] `temperature: 0.0` for consistency  
- [ ] System instructions emphasize exact adherence
- [ ] Using `conversation.item.create` before `response.create`
- [ ] No response cancellation logic
- [ ] DataChannel is open before sending messages
- [ ] Proper error handling and logging

### ❌ Settings to Avoid
- [ ] Don't use `response.create` with instructions parameter
- [ ] Don't use response cancellation (`response.cancel`, `response.stop`)
- [ ] Don't use automatic turn detection (`server_vad`, `semantic_vad`)
- [ ] Don't use high temperature values
- [ ] Don't mix manual and automatic response generation

## Common Issues and Solutions

### Issue: AI still generates original responses
**Solution:** Verify `turn_detection: null` and `temperature: 0.0` in session config

### Issue: Responses are delayed or don't play  
**Solution:** Check setTimeout delays, use 100-200ms between conversation item and response creation

### Issue: Multiple responses playing
**Solution:** Remove all response.cancel logic, use only conversation items

### Issue: Script not exactly matching
**Solution:** Verify the text in `conversation.item.create` matches your script exactly

### Issue: Session configuration not applied
**Solution:** Ensure dataChannel is open before sending `session.update`

## Key Insight

The OpenAI Realtime API documentation states: "Currently this event can't populate assistant audio messages" - but you **can** populate assistant **text** messages, which the API will then convert to audio when you trigger a response.

**The fundamental fix is:**
1. Stop using `response.create` with instructions
2. Start using `conversation.item.create` with assistant messages containing exact scripts  
3. Then use `response.create` to trigger audio generation of those messages

This approach works because the AI will speak the content of assistant messages that are already in the conversation, rather than generating new content based on instructions.

## Implementation Priority

**HIGH PRIORITY (Immediate)**
1. Update session configuration with `turn_detection: null`
2. Replace `createManualResponse` with `createScriptedVoiceResponse`
3. Remove all response cancellation logic

**MEDIUM PRIORITY (Next)**
4. Update state machine integration
5. Implement proper error handling
6. Add comprehensive logging

**LOW PRIORITY (Future)**
7. Add advanced debugging features
8. Optimize timing between conversation items and responses
9. Add fallback mechanisms for edge cases

This fix will provide true script adherence for the Mind Shifting treatment sessions by properly controlling the OpenAI Realtime API's conversation context rather than attempting to override its response generation behavior.
